{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEvl0pj1tBJS",
        "outputId": "670a75ad-4213-43d8-8a5a-abe0b202c39b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2022-10-02 17:25:02--  https://drive.google.com/u/0/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.42.110\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.42.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://drive.google.com/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download [following]\n",
            "--2022-10-02 17:25:03--  https://drive.google.com/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-3o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gjpo2svbcjqgk606t1hki0fahjkmlepo/1664711700000/02008525212197398114/*/1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO?e=download&uuid=287e1d20-8900-4ef4-a674-4196e75b5eef [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-10-02 17:25:05--  https://doc-14-3o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/gjpo2svbcjqgk606t1hki0fahjkmlepo/1664711700000/02008525212197398114/*/1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO?e=download&uuid=287e1d20-8900-4ef4-a674-4196e75b5eef\n",
            "Resolving doc-14-3o-docs.googleusercontent.com (doc-14-3o-docs.googleusercontent.com)... 142.250.194.193\n",
            "Connecting to doc-14-3o-docs.googleusercontent.com (doc-14-3o-docs.googleusercontent.com)|142.250.194.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1122316 (1.1M) [text/plain]\n",
            "Saving to: 'english_python_data.txt'\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  4% 4.09M 0s\n",
            "    50K .......... .......... .......... .......... ..........  9%  261K 2s\n",
            "   100K .......... .......... .......... .......... .......... 13% 22.8M 1s\n",
            "   150K .......... .......... .......... .......... .......... 18% 6.71M 1s\n",
            "   200K .......... .......... .......... .......... .......... 22% 12.0M 1s\n",
            "   250K .......... .......... .......... .......... .......... 27% 22.8M 1s\n",
            "   300K .......... .......... .......... .......... .......... 31% 7.75M 0s\n",
            "   350K .......... .......... .......... .......... .......... 36%  847K 0s\n",
            "   400K .......... .......... .......... .......... .......... 41% 11.3M 0s\n",
            "   450K .......... .......... .......... .......... .......... 45% 24.5M 0s\n",
            "   500K .......... .......... .......... .......... .......... 50% 36.1M 0s\n",
            "   550K .......... .......... .......... .......... .......... 54% 36.6M 0s\n",
            "   600K .......... .......... .......... .......... .......... 59% 8.42M 0s\n",
            "   650K .......... .......... .......... .......... .......... 63% 55.0M 0s\n",
            "   700K .......... .......... .......... .......... .......... 68% 4.35M 0s\n",
            "   750K .......... .......... .......... .......... .......... 72% 27.2M 0s\n",
            "   800K .......... .......... .......... .......... .......... 77% 11.5M 0s\n",
            "   850K .......... .......... .......... .......... .......... 82% 7.31M 0s\n",
            "   900K .......... .......... .......... .......... .......... 86% 10.0M 0s\n",
            "   950K .......... .......... .......... .......... .......... 91% 9.26M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 95% 9.85M 0s\n",
            "  1050K .......... .......... .......... .......... ......    100% 11.0M=0.3s\n",
            "\n",
            "2022-10-02 17:25:06 (3.11 MB/s) - 'english_python_data.txt' saved [1122316/1122316]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://drive.google.com/u/0/uc?id=1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO&export=download\" -O english_python_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "g4vBn1Rpt2YG",
        "outputId": "2c6320d7-8eff-43e4-86c8-56851f528faf"
      },
      "outputs": [],
      "source": [
        "# !pip install torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0vOt7kwytK0M"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\docto\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy.data import *\n",
        "from torchtext.legacy import data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XPPgLtQvtwQW",
        "outputId": "8c62d727-1248-4b80-fe4a-95541e22b50a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.10.0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEMcl0NsuoRl",
        "outputId": "629607fb-1458-42b2-b98b-21274668ee65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ]
        }
      ],
      "source": [
        "%set_env CUDA_LAUNCH_BLOCKING = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c74HGvogusuw"
      },
      "outputs": [],
      "source": [
        "f = open(\"./english_python_data.txt\",\"r\",encoding='utf8')\n",
        "lines = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzcyl2O0u3Ig",
        "outputId": "97b29ab5-52fa-425d-cd7b-39f71ff4faf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['# write a python program to add two numbers \\n',\n",
              " 'num1 = 1.5\\n',\n",
              " 'num2 = 6.3\\n',\n",
              " 'sum = num1 + num2\\n',\n",
              " \"print(f'Sum: {sum}')\\n\",\n",
              " '\\n',\n",
              " '\\n',\n",
              " '# write a python function to add two user provided numbers and return the sum\\n',\n",
              " 'def add_two_numbers(num1, num2):\\n',\n",
              " '    sum = num1 + num2\\n',\n",
              " '    return sum\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '# write a program to find and print the largest among three numbers\\n',\n",
              " '\\n']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TbG2iw3Ju5Ty"
      },
      "outputs": [],
      "source": [
        "dps = []\n",
        "dp = None\n",
        "\n",
        "for line in lines:\n",
        "    if line[0] == '#':\n",
        "        if dp:\n",
        "            dp['solution'] = ' '.join(dp['solution'])\n",
        "            dps.append(dp)\n",
        "        dp = {\n",
        "            \"question\":None,\n",
        "            \"solution\":[]\n",
        "            }\n",
        "        dp['question'] = line[1:]\n",
        "    else:\n",
        "        dp['solution'].append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdyMVzPfvf2N",
        "outputId": "87aadd9c-c445-417f-ce8a-4f525a4acbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Question # 1\n",
            "write a python program to add two numbers \n",
            "\n",
            "num1 = 1.5\n",
            " num2 = 6.3\n",
            " sum = num1 + num2\n",
            " print(f'Sum: {sum}')\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 2\n",
            "write a python function to add two user provided numbers and return the sum\n",
            "\n",
            "def add_two_numbers(num1, num2):\n",
            "     sum = num1 + num2\n",
            "     return sum\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 3\n",
            "write a program to find and print the largest among three numbers\n",
            "\n",
            "\n",
            " num1 = 10\n",
            " num2 = 12\n",
            " num3 = 14\n",
            " if (num1 >= num2) and (num1 >= num3):\n",
            "    largest = num1\n",
            " elif (num2 >= num1) and (num2 >= num3):\n",
            "    largest = num2\n",
            " else:\n",
            "    largest = num3\n",
            " print(f'largest:{largest}')\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 4\n",
            "write a program to find and print the smallest among three numbers\n",
            "\n",
            "num1 = 10\n",
            " num2 = 12\n",
            " num3 = 14\n",
            " if (num1 <= num2) and (num1 <= num3):\n",
            "    smallest = num1\n",
            " elif (num2 <= num1) and (num2 <= num3):\n",
            "    smallest = num2\n",
            " else:\n",
            "    smallest = num3\n",
            " print(f'smallest:{smallest}')\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 5\n",
            "Write a python function to merge two given lists into one\n",
            "\n",
            "def merge_lists(l1, l2):\n",
            "     return l1 + l2\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 6\n",
            "Write a program to check whether a number is prime or not\n",
            "\n",
            "num = 337\n",
            " \n",
            " if num > 1:\n",
            "    for i in range(2, num//2 + 1):\n",
            "        if (num % i) == 0:\n",
            "            print(num,\"is not a prime number\")\n",
            "            print(f\"{i} times {num//i} is {num}\")\n",
            "            break\n",
            "    else:\n",
            "        print(f\"{num} is a prime number\")\n",
            " \n",
            " else:\n",
            "    print(f\"{num} is not a prime number\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 7\n",
            "Write a python function that prints the factors of a given number\n",
            "\n",
            "def print_factors(x):\n",
            "    print(f\"The factors of {x} are:\")\n",
            "    for i in range(1, x + 1):\n",
            "        if x % i == 0:\n",
            "            print(i)\n",
            " \n",
            "\n",
            "\n",
            " Question # 8\n",
            "Write a program to find the factorial of a number\n",
            "\n",
            "num = 13\n",
            " factorial = 1\n",
            " \n",
            " if num < 0:\n",
            "    print(\"No factorials for negative numbers!\")\n",
            " \n",
            " elif num == 0:\n",
            "    print(\"The factorial of 0 is 1\")\n",
            " \n",
            " else:\n",
            "    for i in range(1,num + 1):\n",
            "        factorial = factorial*i\n",
            "    print(f\"The factorial of {num} is {factorial}\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 9\n",
            "Write a python function to print whether a number is negative, positive or zero\n",
            "\n",
            "def check_pnz(num):\n",
            "     if num > 0:\n",
            "        print(\"Positive number\")\n",
            " \n",
            "     elif num == 0:\n",
            "        print(\"Zero\")\n",
            " \n",
            "     else:\n",
            "        print(\"Negative number\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 10\n",
            "Write a program to print the multiplication table of a given number\n",
            "\n",
            "\n",
            " num = 9\n",
            " for i in range(1, 11):\n",
            "    print(f\"{num} x {i} = {num*i}\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 11\n",
            "Write a python function to print powers of 2, for given number of terms\n",
            "\n",
            "def two_power(terms):\n",
            "     result = list(map(lambda x: 2 ** x, range(terms)))\n",
            " \n",
            "     print(f\"The total terms are: {terms}\")\n",
            "     for i in range(terms):\n",
            "        print(f\"2^{i} = {result[i]}\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 12\n",
            "Write a program to filter the numbers in a list which are divisible by a given number\n",
            "\n",
            "my_list = [11, 45, 74, 89, 132, 239, 721, 21]\n",
            " \n",
            " num = 3\n",
            " result = list(filter(lambda x: (x % num == 0), my_list))\n",
            " \n",
            " print(f\"Numbers divisible by {num} are {result}\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 13\n",
            "Write a python function that returns the sum of n natural numbers\n",
            "\n",
            "def sum_natural(num):\n",
            "     if num < 0:\n",
            "        print(\"Please enter a positive number!\")\n",
            "     else:\n",
            "        sum = 0\n",
            "        while(num > 0):\n",
            "            sum += num\n",
            "            num -= 1\n",
            "        return num\n",
            " \n",
            "\n",
            "\n",
            " Question # 14\n",
            "Write a program to swap first and last elements in a list\n",
            "\n",
            "my_list = [1, 2, 3, 4, 5, 6]\n",
            " my_list[0], my_list[-1] = my_list[-1], my_list[0]\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 15\n",
            "Write a python function to find the area of a circle, whose radius is given\n",
            "\n",
            "def findArea(r): \n",
            "     PI = 3.142\n",
            "     return PI * (r*r)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 16\n",
            "Write a program to print the sum of squares of first n natural numbers\n",
            "\n",
            "n = 21\n",
            " sum_n = 0\n",
            " for i in range(1, n+1):\n",
            "     sum_n += i**2\n",
            " print(sum_n)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 17\n",
            "Write a program to print the length of a list\n",
            "\n",
            "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " \n",
            " print(len(my_list))\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 18\n",
            "Write a pythno function to print the length of a given tuple\n",
            "\n",
            "my_tuple = (1, 2, 3, 4, 5, 6, 7, 8)\n",
            " \n",
            " print(len(my_tuple))\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 19\n",
            "Write a python function to print the elements of a given list, one element in a line\n",
            "\n",
            "def custom_print(l):\n",
            "     for _ in l:\n",
            "         print(_)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 20\n",
            "Write a python function to remove all the odd numbers from a list and return the remaining list\n",
            "\n",
            "\n",
            " def remove_odd(my_list):\n",
            "     result = list(filter(lambda x: (x % 2 == 0), my_list))\n",
            "     return result\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 21\n",
            "Write a python function to remove all the even numbers from a list and return the remaining list\n",
            "\n",
            "\n",
            " def remove_even(my_list):\n",
            "     result = list(filter(lambda x: (x % 2 != 0), my_list))\n",
            "     return result\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 22\n",
            "Write a function that takes two lists as input and returns a zipped list of corresponding elements\n",
            "\n",
            "\n",
            " def zip_list(list1, list2):\n",
            "     return list(zip(list1, list2))\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 23\n",
            "Write a program to to print the contents of a given file\n",
            "\n",
            "file_name = 'temp.txt'\n",
            " with open(file_name, 'r') as f:\n",
            "     print(f.read())\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 24\n",
            "Write a functin that returns the LCM of two input numbers\n",
            "\n",
            "\n",
            " def lcm(a, b):\n",
            "     if a>b:\n",
            "         min_ = a\n",
            "     else:\n",
            "         min_ = b\n",
            "     while True:\n",
            "         if min_%a==0 and min_%b==0:\n",
            "             break\n",
            "         min_+=1\n",
            "     return min_\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 25\n",
            "Write a program to print the unique elements in a list\n",
            "\n",
            "my_list = [1, 2, 4, 5, 2, 3, 1, 5, 4, 7, 8, 2, 4, 5, 2, 7, 3]\n",
            " \n",
            " print(set(my_list))\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 26\n",
            "Write a function that returns the sum of digits of a given number\n",
            "\n",
            "def digisum(num):\n",
            "     sum_=0\n",
            "     while num > 0:\n",
            "         dig = num % 10\n",
            "         sum_+=dig\n",
            "         num//=10\n",
            "     return sum_\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 27\n",
            "Write a program to check and print whether a number is palindrome or not\n",
            "\n",
            "\n",
            " num = 12321\n",
            " temp = num\n",
            " rev = 0\n",
            " while num > 0:\n",
            "     dig = num % 10\n",
            "     rev = rev*10 + dig\n",
            "     num//=10\n",
            " if temp==rev :\n",
            "     print(\"The number is a palindrome!\")\n",
            " else:\n",
            "     print(\"The number isn't a palindrome!\")\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 28\n",
            "Write a function that prints a given value, n number of times\n",
            "\n",
            "def print_n(val, n):\n",
            "     for _ in range(n):\n",
            "         print(val)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 29\n",
            "Write a function to find the area of sqaure\n",
            "\n",
            "def square_area(a):\n",
            "     return a*a\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 30\n",
            "Write a function to find the perimeter of a square\n",
            "\n",
            "def square_perimeter(a):\n",
            "     return 4*a\n",
            " \n",
            "\n",
            "\n",
            " Question # 31\n",
            "Write a function to find the area of rectangle\n",
            "\n",
            "def rectangle_area(l, b):\n",
            "     return l*b\n",
            " \n",
            "\n",
            "\n",
            " Question # 32\n",
            "Write a function to find the permieter of a rectangle\n",
            "\n",
            "def rectangle_perimeter(l, b):\n",
            "     return 2*(l+b)\n",
            " \n",
            "\n",
            "\n",
            " Question # 33\n",
            "Write a python function to find the area of a circle, whose radius is given\n",
            "\n",
            "def findArea(r): \n",
            "     PI = 3.142\n",
            "     return PI * (r*r)\n",
            " \n",
            "\n",
            "\n",
            " Question # 34\n",
            "Write a function to calculate and return electricity bill. Units used are given. Price per unit is fixed and is increased after 750 units.\n",
            "\n",
            "\n",
            " def calc_elect_bill(units):\n",
            "     if units > 0:\n",
            "         if units <= 750:\n",
            "             return 5*units\n",
            "         else:\n",
            "             return 5*(750) + 7*(units-750)\n",
            " \n",
            "     else:\n",
            "         return -1\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 35\n",
            "Write a function to return day of a week, given the number\n",
            "\n",
            "def give_day(n):\n",
            "     day_dict = {1: 'Sunday', 2: 'Monday', 3: 'Tuesday', 4: 'Wednesday', 5: 'Thursday', 6: 'Friday', 7: 'Saturday'}\n",
            "     return day_dict[n]\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 36\n",
            "Write a program to calculate and print the volume of a cylender\n",
            "\n",
            "r = 3\n",
            " h = 5\n",
            " pi = 3.14\n",
            " volume = pi*(r**2)*h\n",
            " print(volume)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 37\n",
            "Write a function to calculate and return the average of input numbers\n",
            "\n",
            "\n",
            " def calc_avg(*args):\n",
            "     if len(args) > 0:\n",
            "         return sum(args)/len(args)\n",
            "     return None\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 38\n",
            "Write a function to calculate compound interest, given p, r, t\n",
            "\n",
            "def comp_int(p, r, t):\n",
            "     amount = p * (1 + (r/100))**t\n",
            "     interest = amount - p\n",
            "     return interest\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 39\n",
            "Write a function to calculate simple interest, given p, r, t\n",
            "\n",
            "def simp_int(p, r, t):\n",
            "     interest = (p*r*t)/100\n",
            "     return interest\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 40\n",
            "Write a program to print a given string, replacing all the vowels with '_'\n",
            "\n",
            "\n",
            " st = \"Where is this going? Could you please help me understand!\"\n",
            " vowels = \"AEIOUaeiou\"\n",
            " \n",
            " for v in vowels:\n",
            "     st = st.replace(v, '_')\n",
            " \n",
            " print(st)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 41\n",
            "Write a functio to check whether a number if perfect or not\n",
            "\n",
            "def is_perfect(n):\n",
            "     sum_ = 0\n",
            "     for i in range(1, n//2 + 1):\n",
            "         if n%i == 0:\n",
            "             sum_+=i\n",
            "     if sum_ == n:\n",
            "         return True\n",
            "     return False\n",
            " \n",
            "\n",
            "\n",
            " Question # 42\n",
            "Write a function that returns seperate lists of positive and negative numbers from an input list\n",
            "\n",
            "def seperate_pn(l):\n",
            "     pos_list = []\n",
            "     neg_list = []\n",
            "     for _ in l:\n",
            "         if _<0:\n",
            "             neg_list.append(_)\n",
            "         else:\n",
            "             pos_list.append(_)\n",
            "     return pos_list, neg_list\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 43\n",
            "Write a program to find and print the area of a triangle, whose hight and width are given.\n",
            "\n",
            "\n",
            " h = 12\n",
            " w = 11\n",
            " area = 0.5*h*w\n",
            " print(area)\n",
            " \n",
            " \n",
            "\n",
            "\n",
            " Question # 44\n",
            "Write a function to find acceleration, given u, v and t\n",
            "\n",
            "\n",
            " def acc(u, v, t):\n",
            "     return (v-u)/t\n",
            " \n",
            "\n",
            "\n",
            " Question # 45\n",
            "Write a lambda function to multiply two numbers\n",
            "\n",
            "\n",
            " multiply = lambda a, b: a*b\n",
            " \n",
            "\n",
            "\n",
            " Question # 46\n",
            "Write a lambda function to add two numbers\n",
            "\n",
            "\n",
            " add = lambda a, b: a+b\n",
            " \n",
            "\n",
            "\n",
            " Question # 47\n",
            "Write a lambda function that gives True if the input number is even otherwise False\n",
            "\n",
            "\n",
            " even = lambda a: True if a%2 == 0 else False\n",
            " \n",
            "\n",
            "\n",
            " Question # 48\n",
            "Write a lambda function to to give character grom it's ascii value\n",
            "\n",
            "\n",
            " ascii = lambda a: chr(a)\n",
            " \n",
            "\n",
            "\n",
            " Question # 49\n",
            "Write a lambda function to that gives the number of digits in a number\n",
            "\n",
            "\n",
            " dig_cnt = lambda a: len(str(a))\n",
            " \n",
            "\n",
            "\n",
            " Question # 50\n",
            "Write a program to to check if a triangle is valid or not, given it's all three angles\n",
            "\n",
            "\n",
            " def is_valid_triangle_angle(a, b c):\n",
            "     if a+b+c == 180:\n",
            "         return True\n",
            "     return False\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for dp in dps:\n",
        "    print(\"\\n Question #\",i+1)\n",
        "    i+=1\n",
        "    print(dp['question'][1:])\n",
        "    print(dp['solution'])\n",
        "    if i>49:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlTXGMa5vyNr",
        "outputId": "9a2a8ca3-6fb0-4e7d-c9f6-7831ce38de06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size:  4957\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset size: \", len(dps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "30G3Ja1sv_bh"
      },
      "outputs": [],
      "source": [
        "from tokenize import tokenize, untokenize\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EyzH4UVqwFe5"
      },
      "outputs": [],
      "source": [
        "def tokenizePythonCode(pythonCodeStr):\n",
        "    pythonTokens = list(tokenize(io.BytesIO(pythonCodeStr.encode('utf-8')).readline))\n",
        "    tokenizeOutput = []\n",
        "    for i in range(0,len(pythonTokens)):\n",
        "        tokenizeOutput.append((pythonTokens[i].type, pythonTokens[i].string))\n",
        "    return tokenizeOutput\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr4Z1IKqwrhe",
        "outputId": "704908a9-e7cc-43f4-feca-2bdf24eefc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(57, 'utf-8'), (1, 'def'), (1, 'add_two_numbers'), (53, '('), (1, 'num1'), (53, ','), (1, 'num2'), (53, ')'), (53, ':'), (4, '\\n'), (5, '     '), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'num2'), (4, '\\n'), (1, 'return'), (1, 'sum'), (4, '\\n'), (56, '\\n'), (56, '\\n'), (6, ''), (0, '')]\n"
          ]
        }
      ],
      "source": [
        "tokenizeSample = tokenizePythonCode(dps[1]['solution'])\n",
        "print(tokenizeSample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hF8ZFvn7w0OX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def add_two_numbers (num1 ,num2 ):\n",
            "     sum =num1 +num2 \n",
            "     return sum \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(untokenize(tokenizeSample).decode('utf8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield']\n"
          ]
        }
      ],
      "source": [
        "import keyword\n",
        "print(keyword.kwlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augmentTokenizePythonCode(pythonCodeStr,maskFactor=0.3):\n",
        "    var_dict = {}\n",
        "    skip_list = ['range', 'enumerate', 'print', 'ord', 'int', 'float', 'zip', 'char', 'list', 'dict', 'tuple', 'set', 'len', 'sum', 'min', 'max']\n",
        "    skip_list.extend(keyword.kwlist)\n",
        "\n",
        "    var_counter = 1\n",
        "    python_tokens = list(tokenize(io.BytesIO(pythonCodeStr.encode('utf8')).readline))\n",
        "    tokenize_output = []\n",
        "\n",
        "    for i in range(0, len(python_tokens)):\n",
        "        if python_tokens[i].type == 1 and python_tokens[i].string not in skip_list:\n",
        "            if i>0 and python_tokens[i-1].string in ['def', '.', 'import', 'raise', 'except', 'class']:\n",
        "                skip_list.append(python_tokens[i].string)\n",
        "                tokenize_output.append((python_tokens[i].type,python_tokens[i].string))\n",
        "            elif python_tokens[i].string in var_dict:\n",
        "                tokenize_output.append((python_tokens[i].type, var_dict[python_tokens[i].string]))\n",
        "            elif random.uniform(0,1) > 1-maskFactor:\n",
        "                var_dict[python_tokens[i].string] = 'var_' + str(var_counter)\n",
        "                var_counter+=1\n",
        "                tokenize_output.append((python_tokens[i].type,python_tokens[i].string))\n",
        "            else:\n",
        "                skip_list.append(python_tokens[i].string)\n",
        "                tokenize_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "        else:\n",
        "            tokenize_output.append((python_tokens[i].type, python_tokens[i].string))\n",
        "    return tokenize_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(57, 'utf-8'), (1, 'def'), (1, 'add_two_numbers'), (53, '('), (1, 'num1'), (53, ','), (1, 'num2'), (53, ')'), (53, ':'), (4, '\\n'), (5, '     '), (1, 'sum'), (53, '='), (1, 'var_1'), (53, '+'), (1, 'var_2'), (4, '\\n'), (1, 'return'), (1, 'sum'), (4, '\\n'), (56, '\\n'), (56, '\\n'), (6, ''), (0, '')]\n"
          ]
        }
      ],
      "source": [
        "tokenizeSample = augmentTokenizePythonCode(dps[1]['solution'])\n",
        "print(tokenizeSample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "def add_two_numbers (num1 ,num2 ):\n",
            "     sum =var_1 +var_2 \n",
            "     return sum \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(untokenize(tokenizeSample).decode('utf8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "python_problems = pd.DataFrame(dps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>write a python program to add two numbers \\n</td>\n",
              "      <td>num1 = 1.5\\n num2 = 6.3\\n sum = num1 + num2\\n ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>write a python function to add two user provi...</td>\n",
              "      <td>def add_two_numbers(num1, num2):\\n     sum = n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>write a program to find and print the largest...</td>\n",
              "      <td>\\n num1 = 10\\n num2 = 12\\n num3 = 14\\n if (num...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>write a program to find and print the smalles...</td>\n",
              "      <td>num1 = 10\\n num2 = 12\\n num3 = 14\\n if (num1 &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Write a python function to merge two given li...</td>\n",
              "      <td>def merge_lists(l1, l2):\\n     return l1 + l2\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0       write a python program to add two numbers \\n   \n",
              "1   write a python function to add two user provi...   \n",
              "2   write a program to find and print the largest...   \n",
              "3   write a program to find and print the smalles...   \n",
              "4   Write a python function to merge two given li...   \n",
              "\n",
              "                                            solution  \n",
              "0  num1 = 1.5\\n num2 = 6.3\\n sum = num1 + num2\\n ...  \n",
              "1  def add_two_numbers(num1, num2):\\n     sum = n...  \n",
              "2  \\n num1 = 10\\n num2 = 12\\n num3 = 14\\n if (num...  \n",
              "3  num1 = 10\\n num2 = 12\\n num3 = 14\\n if (num1 <...  \n",
              "4  def merge_lists(l1, l2):\\n     return l1 + l2\\...  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4957, 2)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "python_problems.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "msk = np.random.rand(len(python_problems)) < 0.85\n",
        "train_df = python_problems[msk]\n",
        "val_df = python_problems[~msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4211, 2) (746, 2)\n"
          ]
        }
      ],
      "source": [
        "print(train_df.shape,val_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 6969\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\docto\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        }
      ],
      "source": [
        "Input = data.Field(tokenize='spacy',init_token='<sos>',eos_token='<eos>',lower=True)\n",
        "Output = data.Field(tokenize=augmentTokenizePythonCode, init_token='<sos>', eos_token='<eos>', lower=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "fields = [('Input',Input),('Output',Output)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_example = []\n",
        "val_example = []\n",
        "\n",
        "train_expansion_factor = 100\n",
        "for j in range(train_expansion_factor):\n",
        "    for i in range(train_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([train_df.question[i],train_df.solution[i]],fields)\n",
        "            train_example.append(ex)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "for i in range(val_df.shape[0]):\n",
        "    try:\n",
        "        ex = data.Example.fromlist([val_df.question[i],val_df.solution[i]],fields)\n",
        "        val_example.append(ex)\n",
        "    except Exception as e:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = data.Dataset(train_example,fields)\n",
        "valid_data = data.Dataset(val_example,fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "Input.build_vocab(train_data,min_freq=0)\n",
        "Output.build_vocab(train_data,min_freq=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torchtext.legacy.vocab.Vocab object at 0x000001EA995E49C8>\n"
          ]
        }
      ],
      "source": [
        "print(Output.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def saveVocab(vocab,path):\n",
        "    import pickle\n",
        "    output = open(path,'wb')\n",
        "    pickle.dump(vocab,output)\n",
        "    output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(57, 'utf-8'),\n",
              " (1, 'num1'),\n",
              " (53, '='),\n",
              " (2, '1.5'),\n",
              " (4, '\\n'),\n",
              " (5, ' '),\n",
              " (1, 'num2'),\n",
              " (53, '='),\n",
              " (2, '6.3'),\n",
              " (4, '\\n'),\n",
              " (1, 'sum'),\n",
              " (53, '='),\n",
              " (1, 'num1'),\n",
              " (53, '+'),\n",
              " (1, 'var_1'),\n",
              " (4, '\\n'),\n",
              " (1, 'print'),\n",
              " (53, '('),\n",
              " (3, \"f'Sum: {sum}'\"),\n",
              " (53, ')'),\n",
              " (4, '\\n'),\n",
              " (56, '\\n'),\n",
              " (6, ''),\n",
              " (0, '')]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0].Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Input': [' ', 'write', 'a', 'python', 'function', 'to', 'add', 'two', 'user', 'provided', 'numbers', 'and', 'return', 'the', 'sum'], 'Output': [(57, 'utf-8'), (1, 'def'), (1, 'add_two_numbers'), (53, '('), (1, 'num1'), (53, ','), (1, 'num2'), (53, ')'), (53, ':'), (4, '\\n'), (5, '     '), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'num2'), (4, '\\n'), (1, 'return'), (1, 'sum'), (4, '\\n'), (56, '\\n'), (6, ''), (0, '')]}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length=1000):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim,hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length,hid_dim)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "    \n",
        "    def forward(self, src, src_mask):\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0,src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
        "        src = self.dropout((self.tok_embedding(src)*self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src,src_mask)\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
        "        self.positionwise_feedforward = PositionWiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src,src_mask):\n",
        "        _src, _ = self.self_attention(src,src,src,src_mask)\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionWiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self,hid_dim,pf_dim,dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hid_dim,pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim,hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self,hid_dim,n_heads,dropout,device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hid_dim%n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim//n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(hid_dim,hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim,hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim,hid_dim)\n",
        "        self.fc_o = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "    def forward(self,query,key,value,mask = None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        Q = Q.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "        K = K.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "        V = V.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "\n",
        "        energy = torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy,dim = -1)\n",
        "        \n",
        "        x = torch.matmul(self.dropout(attention),V)\n",
        "        x = x.permute(0,2,1,3).contiguous()\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        x = self.fc_o(x)\n",
        "        return x,attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import FloatTensor, unsqueeze\n",
        "from tqdm import tgrange\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim,hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length,hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(hid_dim,output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self,trg,enc_src,trg_mask,src_mask):\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size,1).to(device)\n",
        "        trg = self.dropout((self.tok_embedding(trg)*self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            trg,attention = layer(trg,enc_src,trg_mask,src_mask)\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        return output,attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,hid_dim,n_heads,pf_dim,dropout,device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionWiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,trg,enc_src,trg_mask,src_mask):\n",
        "        _trg, _ = self.self_attention(trg,trg,trg,trg_mask)\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        _trg, attention = self.encoder_attention(trg,enc_src,enc_src,src_mask)\n",
        "        trg = self.enc_attn_layer_norm(trg+self.dropout(_trg))\n",
        "\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        trg = self.ff_layer_norm(trg+self.dropout(_trg))\n",
        "\n",
        "        return trg, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,src_pad_idx,trg_pad_idx,device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def make_src_mask(self,src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "    def make_trg_mask(self,trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len), device = self.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        return trg_mask\n",
        "    \n",
        "    def forward(self,src,trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src,src_mask)\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = len(Input.vocab)\n",
        "OUTPUT_DIM = len(Output.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 16\n",
        "DEC_HEADS = 16\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM,HID_DIM,ENC_LAYERS,ENC_HEADS,ENC_PF_DIM,ENC_DROPOUT,device)\n",
        "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5386"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(Output.vocab.__dict__['freqs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "SRC_PAD_IDX = Input.vocab.stoi[Input.pad_token]\n",
        "TRG_PAD_IDX = Output.vocab.stoi[Output.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 10,059,790 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m,'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(2051, 256)\n",
              "    (pos_embedding): Embedding(1000, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(5390, 256)\n",
              "    (pos_embedding): Embedding(10000, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionWiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=5390, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossEntropyLoss(nn.CrossEntropyLoss):\n",
        "\n",
        "    def __init__(self, weight=None, ignore_index=-100, reduction='mean', smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "        super(CrossEntropyLoss, self).__init__(weight=weight, ignore_index=ignore_index, reduction=reduction)\n",
        "        self.smooth_eps = smooth_eps\n",
        "        self.smooth_dist = smooth_dist\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def forward(self, input, target, smooth_dist=None):\n",
        "        if smooth_dist is None:\n",
        "            smooth_dist = self.smooth_dist\n",
        "        return cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index,\n",
        "                             reduction=self.reduction, smooth_eps=self.smooth_eps,\n",
        "                             smooth_dist=smooth_dist, from_logits=self.from_logits)\n",
        "\n",
        "\n",
        "def cross_entropy(inputs, target, weight=None, ignore_index=-100, reduction='mean', smooth_eps=None, smooth_dist=None, from_logits=True):\n",
        "    smooth_eps = smooth_eps or 0\n",
        "\n",
        "    if _is_long(target) and smooth_eps == 0:\n",
        "        if from_logits:\n",
        "            return F.cross_entropy(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "        else:\n",
        "            return F.nll_loss(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    if from_logits:\n",
        "        # log-softmax of inputs\n",
        "        lsm = F.log_softmax(inputs, dim=-1)\n",
        "    else:\n",
        "        lsm = inputs\n",
        "\n",
        "    masked_indices = None\n",
        "    num_classes = inputs.size(-1)\n",
        "\n",
        "    if _is_long(target) and ignore_index >= 0:\n",
        "        masked_indices = target.eq(ignore_index)\n",
        "\n",
        "    if smooth_eps > 0 and smooth_dist is not None:\n",
        "        if _is_long(target):\n",
        "            target = onehot(target, num_classes).type_as(inputs)\n",
        "        if smooth_dist.dim() < target.dim():\n",
        "            smooth_dist = smooth_dist.unsqueeze(0)\n",
        "        target.lerp_(smooth_dist, smooth_eps)\n",
        "\n",
        "    if weight is not None:\n",
        "        lsm = lsm * weight.unsqueeze(0)\n",
        "\n",
        "    if _is_long(target):\n",
        "        eps_sum = smooth_eps / num_classes\n",
        "        eps_nll = 1. - eps_sum - smooth_eps\n",
        "        likelihood = lsm.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n",
        "        loss = -(eps_nll * likelihood + eps_sum * lsm.sum(-1))\n",
        "    else:\n",
        "        loss = -(target * lsm).sum(-1)\n",
        "\n",
        "    if masked_indices is not None:\n",
        "        loss.masked_fill_(masked_indices, 0)\n",
        "\n",
        "    if reduction == 'sum':\n",
        "        loss = loss.sum()\n",
        "    elif reduction == 'mean':\n",
        "        if masked_indices is None:\n",
        "            loss = loss.mean()\n",
        "        else:\n",
        "            loss = loss.sum() / float(loss.size(0) - masked_indices.sum())\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def onehot(indexes, N=None, ignore_index=None):\n",
        "    if N is None:\n",
        "        N = indexes.max() + 1\n",
        "    sz = list(indexes.size())\n",
        "    output = indexes.new().byte().resize_(*sz, N).zero_()\n",
        "    output.scatter_(-1, indexes.unsqueeze(-1), 1)\n",
        "    if ignore_index is not None and ignore_index >= 0:\n",
        "        output.masked_fill_(indexes.eq(ignore_index).unsqueeze(-1), 0)\n",
        "    return output\n",
        "\n",
        "def _is_long(x):\n",
        "    if hasattr(x, 'data'):\n",
        "        x = x.data\n",
        "    return isinstance(x, torch.LongTensor) or isinstance(x, torch.cuda.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp,target,mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = CrossEntropyLoss(ignore_index=TRG_PAD_IDX,smooth_eps=0.2)\n",
        "    loss = crossEntropy(inp, target)\n",
        "    loss = loss.to(device)\n",
        "    return loss,nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = maskNLLLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def make_trg_mask(trg):\n",
        "    trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len,trg_len),device=device)).bool()\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "    return trg_mask\n",
        "\n",
        "def train(model,iterator,optimizer,criterion,clip):\n",
        "    model.train()\n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "\n",
        "    for i,batch in tqdm(enumerate(iterator),total=len(iterator)):\n",
        "        loss = 0\n",
        "        src = batch.Input.permute(1,0)\n",
        "        trg = batch.Output.permute(1,0)\n",
        "\n",
        "        trg_mask = make_trg_mask(trg)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _ = model(src,trg[:,:-1])\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1,output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        mask_loss, nTotal = criterion(output,trg,trg_mask)\n",
        "        mask_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        print_losses.append(mask_loss.item()*nTotal)\n",
        "        n_totals+=nTotal\n",
        "\n",
        "    return sum(print_losses)/n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model,iterator,criterion):\n",
        "    model.eval()\n",
        "    n_totals = 0\n",
        "    print_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i,batch in tqdm(enumerate(iterator),total=len(iterator)):\n",
        "            src = batch.Input.permute(1,0)\n",
        "            trg = batch.Output.permute(1,0)\n",
        "            trg_mask = make_trg_mask(trg)\n",
        "\n",
        "            output, _ = model(src,trg[:,:-1])\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1,output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            mask_loss, nTotal = criterion(output,trg,trg_mask)\n",
        "            print_losses.append(mask_loss.item()*nTotal)\n",
        "            n_totals += nTotal\n",
        "    return sum(print_losses)/n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epoch_time(start_time,end_time):\n",
        "    elapsed_time = end_time-start_time\n",
        "    elapsed_mins = int(elapsed_time/60)\n",
        "    elapsed_secs = int(elapsed_time-(elapsed_mins*60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|        | 42/212 [01:29<06:02,  2.13s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1892\\2371879421.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                                                                 sort_within_batch=True, device = device)\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1892\\3533016572.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mmask_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnTotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mmask_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1892\\3690111629.py\u001b[0m in \u001b[0;36mmaskNLLLoss\u001b[1;34m(inp, target, mask)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnTotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcrossEntropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRG_PAD_IDX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmooth_eps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossEntropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnTotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\docto\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1892\\197441147.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target, smooth_dist)\u001b[0m\n\u001b[0;32m     12\u001b[0m         return cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index,\n\u001b[0;32m     13\u001b[0m                              \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_eps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmooth_eps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                              smooth_dist=smooth_dist, from_logits=self.from_logits)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1892\\197441147.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(inputs, target, weight, ignore_index, reduction, smooth_eps, smooth_dist, from_logits)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# log-softmax of inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mlsm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mlsm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\docto\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1766\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_example = []\n",
        "    val_example = []\n",
        "\n",
        "    for i in range(train_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([train_df.question[i], train_df.solution[i]], fields)\n",
        "            train_example.append(ex)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for i in range(val_df.shape[0]):\n",
        "        try:\n",
        "            ex = data.Example.fromlist([val_df.question[i], val_df.solution[i]], fields)\n",
        "            val_example.append(ex)\n",
        "        except:\n",
        "            pass       \n",
        "\n",
        "    train_data = data.Dataset(train_example, fields)\n",
        "    valid_data =  data.Dataset(val_example, fields)\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    train_iterator, valid_iterator = BucketIterator.splits((train_data, valid_data), batch_size = BATCH_SIZE, \n",
        "                                                                sort_key = lambda x: len(x.Input),\n",
        "                                                                sort_within_batch=True, device = device)\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/TheSchoolOfAI/EndCapstone/model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "9e9b47167918b8056cbcdadb504d28bf4a60775d7f5bbc39f8a38413d453c738"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
